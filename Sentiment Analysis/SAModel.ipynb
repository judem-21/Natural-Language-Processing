{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxQJExvFKtzr+T2EVC1eRr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/judem-21/Natural-Language-Processing/blob/main/Sentiment%20Analysis/SAModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SA Model"
      ],
      "metadata": {
        "id": "X52qYWWSiitB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing Libraries"
      ],
      "metadata": {
        "id": "YIiccYrnj9Hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kY32_xoMVAPv",
        "outputId": "390cc63b-2b14-49b6-8376-0165c98d17ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.2/491.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/183.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/143.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from datasets import load_dataset\n",
        "from transformers import DataCollatorWithPadding\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from torch.optim.lr_scheduler import StepLR"
      ],
      "metadata": {
        "id": "WgnAFiKeXVk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loading and Preprocessing the Dataset"
      ],
      "metadata": {
        "id": "az3PZ6RTkCHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"Sp1786/multiclass-sentiment-analysis-dataset\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365,
          "referenced_widgets": [
            "9236e3d6d2394a8ea4ac465fe0a8039a",
            "2a452a7b17714d10bc4ffd140aae500a",
            "0042b857aac244efb7a57890ce413fa9",
            "566343b15d3f4b48a958344ac3cfad90",
            "931fe790004848e5832b712c99f86bfe",
            "c4ed03a8c10248cbb884294c5fb22535",
            "5b0fd928c78148818e8d46382d71504c",
            "7b358c36f5a442fcaa619e441be88987",
            "492fcfe2a38f4f54b6bfc7ab76d69fbc",
            "477aaab060824f2996d562ce628cb317",
            "a94a05ecf3a94e7f8cebeb9384861c43",
            "92e731b25c654260804f09d5bb4c306c",
            "485df0aa8fda4ba59eb3847163f4af6e",
            "7e80974ffed6413c968f901be1e9327a",
            "abffd44beba84501b16c60e2425473cb",
            "96a6162f6782406c8149ead992fc4787",
            "c5cadbb1accc4804aeb5f6e0a9b16855",
            "d979ff3a4ea74d31aad20e2d7bbfc2ea",
            "aee59480cc1d43c0a81dcbedc369cace",
            "dfc685b5b42b4b74a4b7f0177cefa2c2",
            "53966860b99d4d8eaf91ea926a3206fc",
            "99b7d02d1aaf402c92708e6b267f4df9",
            "d386297026554ce38a62eaf0b53df8d8",
            "0ccf53e629bc4d10b0e1d5129b5a32c0",
            "aa46797d928a4fb0956b0be209555c99",
            "0aabbd96adb44cab8ce087bfb51df28f",
            "f3c0e9e151984233a89b58d6e17161c7",
            "9f10fc4096854a848c12a9cb4deb2b02",
            "86c45fd9a287497b9ad9484a1828820f",
            "0bce9c4988d34801bc1765d8ff5377db",
            "51d09498fe6341beae9fcb8144bf72c7",
            "55e1022f02114285a9f49021e1b8ac31",
            "d0a3b1e62f9d4b50a0d991e63ddf6a3a",
            "f3a030be5a3445ce9b5a842a51faeb50",
            "5796b69b4cc9445aa65a56177a85fdfe",
            "fd4536b6f3094060a42f0a4537d5dd39",
            "fd18ea33f47243259e4f2a6f12f6bbfb",
            "b8af00db3e0741debaec3fde864043be",
            "2da682e19443469b8dcdb26affb742a7",
            "d584912c917b4141a0ba24c8c0b4ecc1",
            "6e46e91341984b15b4eb08634f87c95c",
            "7fa1ea2775c445c78552f04a6d3d8bdb",
            "305d4da695f44eacb3fe6e5a1a9ec02f",
            "74fdcf723c8a414fae91be18ae817d45",
            "ec62644efeb545fabe53d2f5b9d1694f",
            "285995d78d56499ea68d31f936d79ee6",
            "6ee03b09e28f46109f95560423e6fd26",
            "c447755986464cdfa93656aef3050901",
            "1164a31e9a554feaa8d75ad62391aaab",
            "c59fa68575b54836b0318a8b0e1ea129",
            "38bfa90ad771479ab71ccf2af207a6ec",
            "78e98dd192254804a8ddecde21df52ae",
            "d17d06a0a3f244fb8700d477ee277cd9",
            "b00e1b039bb04f2891aae376e21b4410",
            "fae7577dd7b046e98d7a1ec677e34650",
            "d989402478a44cb2abe602c1d06b6a80",
            "43fdc89395704906858fd65ec8f83984",
            "b1c137415f9e4ceb902415460a0a6702",
            "f562ba96d198423180ab1f29b6757267",
            "7a67230446034e359ff44be43eb7872b",
            "4c15b2b9d6454a48a3db014a5c0a10de",
            "014e21f1eb944ecb90b018017e5f68a6",
            "ca6e901bb0ef4bd19d34bda347a8bc67",
            "2b8d0ac205b344f3baeeaa5d4a39dc7d",
            "d0d90d78669b4048a13d363d9d2a00b7",
            "7d6300c8b59342d5a3003c28b8e1c8df",
            "078bfa1daa1a4af68e33c4886e075c63",
            "8fb1d731d9fb4979b6a73d2750130eb0",
            "7facb6893aaf4537a99c600f01314205",
            "8bf7163e7679485193ebe2046f651d17",
            "ffee89bd1a2247fa8bdcea0e861a466c",
            "454e78b849e944e0bee24875aaa5e8c4",
            "9990fa1f20f64da484381a30877a5cd9",
            "0303a2ddb4f94126b34a4ace5accdc42",
            "69f5954b5719442ea36eab1fe9a420c0",
            "73fd7ec4203e481ba5bcefd2d8552f0a",
            "756f6425c4ef46e7a9d7ea72c56bab1d"
          ]
        },
        "id": "00aCGwDBuzSR",
        "outputId": "1dcccf30-0231-47c5-f310-b6fd4df32383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9236e3d6d2394a8ea4ac465fe0a8039a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train_df.csv:   0%|          | 0.00/3.56M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92e731b25c654260804f09d5bb4c306c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "val_df.csv:   0%|          | 0.00/601k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d386297026554ce38a62eaf0b53df8d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test_df.csv:   0%|          | 0.00/586k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3a030be5a3445ce9b5a842a51faeb50"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/31232 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec62644efeb545fabe53d2f5b9d1694f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/5205 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d989402478a44cb2abe602c1d06b6a80"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/5206 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "078bfa1daa1a4af68e33c4886e075c63"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "3d92235908d24b838b1dd2b09e37dbf7",
            "229bb15dcd3942f5a20b63f8d84ec772",
            "69b914a14f374760b328d61e7b5924c7",
            "2483be5fc06f4e23b1c270a9b06fd3f4",
            "552c372ca0244bd68dd5b96cc8529748",
            "ac7d7c11e8aa437d8e2e601132f55a1f",
            "488fadf545a64a428ea68665763dd528",
            "5675b4813b88414b8f097abc0f77fc74",
            "c03bba1750334874b828b0214c4b655b",
            "60cd76b213e94d7a99b4012ab8c3ecf8",
            "0058a052755e40b98dcff4b762a854e0",
            "b3f1c46b3d764142a27ac008d7e64a39",
            "22c9e0f63f0d41038d307f8a5698dc67",
            "32f9d7290a7748fe9069c2d5b7346f9e",
            "45267fbe798343abb3c772081d2fecec",
            "83ef8b860ee54cae8e6e4af992c6d800",
            "1cf75e46c59f4c5f9fcab7beb6755190",
            "faa511eae8dc4e2c8493db4444aee04e",
            "0ac77bc17a1a4fb49904d8929e15d5e7",
            "eecb106c1293470eb0f64d1c2132d0e6",
            "c602c94ac8944cefbe49be5458b5fea5",
            "5e8605e8f58e4ee4837af0cfa613edb5",
            "5a832bc8b9b74723bd3a85b1f6499cb3",
            "6f8339f62fa44954ba7b23cf2c5bd74c",
            "a252d28565234b9da5ecd7586c052e17",
            "663da95562f3402aa18f1ea85972da73",
            "7fe1e3d49fd64cd798a740e0a084c3e7",
            "b706adfc4c294047973b99436e02d494",
            "5039a23c142a44cd9eb4fcdbd5ae5f58",
            "d6108d8bc69c48d9a2183864b449475c",
            "5603591c6c204e3c8fc785b3ebf55d16",
            "d4895cfd546d4908b3746d3eaf0bdba1",
            "552110bec78a41c590bb157bd12f2bb6",
            "5ebd1abb021e4c1099771cddda14c34d",
            "e076c94edcb445e9866beb32d10feba6",
            "f80be11d4b1149388a8e99f246209a8b",
            "351350b741844b7e8bfd0a8e28bcbee5",
            "5cbc8076d64a4ec2906c5b44860b543b",
            "9464ecce468b4d5c8cd6b9cfb4900d1e",
            "10a7a7662f004122b2acd4449314423b",
            "5ebc282f78e84ee6a75e7a237ed467b0",
            "cfd37f6a08ad4d57bebb91bcee21286e",
            "16916fe831384307a12797b92ed4e888",
            "0d9e5b84244f4f87b5b7a81114f9dc21"
          ]
        },
        "id": "qGwoxfvYu0nE",
        "outputId": "45fff057-2932-4f37-a7cb-91b6bca29c94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d92235908d24b838b1dd2b09e37dbf7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3f1c46b3d764142a27ac008d7e64a39"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a832bc8b9b74723bd3a85b1f6499cb3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ebd1abb021e4c1099771cddda14c34d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][9]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBGP9qRiHgBH",
        "outputId": "2f7fc300-61d8-4692-a411-ec63e7e4fa5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 793, 'text': '  sad face.', 'label': 0, 'sentiment': 'negative'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_none(example):\n",
        "    return example[\"text\"] is not None\n",
        "\n",
        "dataset['test'] = dataset['test'].filter(filter_none)\n",
        "dataset['validation']=dataset['validation'].filter(filter_none)\n",
        "dataset['train']=dataset['train'].filter(filter_none)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "381f222227ac4c4e9c049e4f24bb9a8d",
            "199dab7068ee4b229b25bc282f642e70",
            "92c28598848249f0975840bff862f027",
            "c23affd37c2f4707968443a7db2614d2",
            "61e9b5ef4f704577a73dc6b67b8106e9",
            "b28c37bd136f478e96f1d9f3d7c4ffa1",
            "497b674315bb481aa17e38374db28d05",
            "68dec44fcb24471ba7f96c4cea130c26",
            "96f1b267ccd846e4b09ca27680f57ba5",
            "9b24aef7e0a94388897c67f01ccb21a6",
            "f3d2efaa05844a0abeb544482c77380a",
            "3b59f6510cde4c948260943374676d10",
            "9058cc3f9de149b2b581691bbc97f29a",
            "9d46502f4b8741ccaa68ce44494c28fd",
            "5398b94d330949349b720218383cca4d",
            "3812ea6a44984722a904db86afc0d0e6",
            "6136e616d75c4d08bf6ed2089f105878",
            "9f114838a4774dee9930f9561516d632",
            "50fe1500096b46aca107bafc1d10369c",
            "d58786561f39405194bf65604939641b",
            "bd84cd4d0adc4bbebd81c8d286e4b036",
            "b7f7d4d66ae042beaedcda25beaad20f",
            "ac42d1235b054e50845ca82c0503dd0a",
            "15b1ba79fd6e4eacbf7b62f40510744a",
            "a1fbaacc28a24b25ade8685e07237dab",
            "4ffa9ab2302f4333a973aee84a1af6c2",
            "7b8352c169b443738c6324832ff2fe16",
            "9e7c5a111232480ebbccd43ad6309801",
            "769d5cc2720c4e0a9e8dbc47b3235a64",
            "b11dc532e0a8492f9a9b7b90b46df973",
            "02465cd3ae094bd982b0159cf94a583b",
            "a7ca88c877e240b6b01391fce4dfb502",
            "fa7aa619cfd84ed89b7e95b960408e2b"
          ]
        },
        "id": "9rAUosrDyRmw",
        "outputId": "205bec23-d8c9-494f-ac6d-b8b6db6ed105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/5206 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "381f222227ac4c4e9c049e4f24bb9a8d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/5205 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b59f6510cde4c948260943374676d10"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/31232 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac42d1235b054e50845ca82c0503dd0a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(batch):\n",
        "    tokenised = tokenizer(text=batch[\"text\"], truncation=True)\n",
        "    tokenised[\"labels\"] = batch[\"label\"]\n",
        "    return tokenised\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(['id', 'text', 'sentiment'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "6bc1a1ecbc4944baacf60d25dc62d588",
            "25959a7a72894799891c9543e80b2919",
            "13bd09c1187c47e3b25ad8ef03c39beb",
            "d2c302a15f494165bedba6dc993df25b",
            "f8fe541637874c3991f978896e4b2502",
            "ac643caf9a22415b9a65bd5e0c94a340",
            "ca719c776f0e4586a81272046f7649e8",
            "6b7937d099de4f1fa2075811c4f21812",
            "9d17726df6b646adbeb4fb60e5ce04f9",
            "c65d9fa558d84cefb13d74a71cb79fa5",
            "a52646b63a0940ec96521bd79173ee27",
            "bd08262d5d90425caadfb16110bca76c",
            "4b30258ffd824eee96a82e1f3dd524f2",
            "2233c0b43364413d83ae48dd70f864b9",
            "9d3e58f7fbab4ca9ac84cf9b44e7460d",
            "253e257795304311a4dcc90ba9f27650",
            "237747e34f3d49a898c9b219a69127d8",
            "60da7bbd527f4ff28fa1bbd7b405a58d",
            "8aa96f98143e42079431a80ca5960f36",
            "7b0482c25a2540aba8658c66c9102b7d",
            "43c2973810fb47fa828fd4aef1230aed",
            "c6c182248d9b45fa9a441e9a6f51b4ce",
            "19ca24217f39417cac745758a0a03e7a",
            "eb51587dfd99444db494058cbe8a96f9",
            "7a964be9d54a44eda44ced43d6e1cea1",
            "c2ecd930159e432c9990ae9da7ccfe61",
            "fd7ce15bdf5e457080f515e06298bf2b",
            "975f1fd7dbab4250a3ae39b0e582ced2",
            "07f3a47b24d549e98d85c44389b81ddd",
            "93beb914aac241a2aeba3e47b4d48032",
            "029786cdce8b45c883a7797b53ad237a",
            "7b8f694c288c4a7f8ee322a88a3a3cdc",
            "632aa7d82e7a4829a17e431f74378fac"
          ]
        },
        "id": "S3AB76Bsu-Hp",
        "outputId": "80274df7-361b-4c92-8fc9-d35fdfea4012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/31232 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6bc1a1ecbc4944baacf60d25dc62d588"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5205 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd08262d5d90425caadfb16110bca76c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5205 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19ca24217f39417cac745758a0a03e7a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
        "train_loader = DataLoader(\n",
        "    tokenized_dataset[\"train\"],\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "val_loader=DataLoader(\n",
        "    tokenized_dataset[\"validation\"],\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "uWW3bxIexSqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in val_loader:\n",
        "    print({k: v.shape for k, v in batch.items()})  # Shows input_ids, attention_mask shapes\n",
        "    print(f'Input Ids:{batch[\"input_ids\"]}')\n",
        "    print(f'Attention Mask:{batch[\"attention_mask\"]}')\n",
        "    print(f'Labels:{batch[\"labels\"]}')\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3-n2PVpzJad",
        "outputId": "105e9150-6592-40f4-f948-cb17c9a40c7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': torch.Size([32, 131]), 'token_type_ids': torch.Size([32, 131]), 'attention_mask': torch.Size([32, 131]), 'labels': torch.Size([32])}\n",
            "Input Ids:tensor([[  101, 10201,  1999,  ...,     0,     0,     0],\n",
            "        [  101,  1051,  9541,  ...,     0,     0,     0],\n",
            "        [  101,  4283,   999,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  3666,  5095,  ...,     0,     0,     0],\n",
            "        [  101,  1035, 10658,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  6287,  ...,     0,     0,     0]])\n",
            "Attention Mask:tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])\n",
            "Labels:tensor([0, 2, 2, 2, 2, 2, 1, 1, 1, 0, 0, 1, 2, 0, 1, 2, 0, 0, 0, 1, 0, 2, 1, 0,\n",
            "        1, 1, 2, 0, 0, 1, 2, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Klz_PT19I8p",
        "outputId": "67a22698-7bf2-4582-940e-c1cf7e580548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Defining the Model"
      ],
      "metadata": {
        "id": "Gqasf2_4kHW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "class SentimentAnalyser(nn.Module):\n",
        "  def __init__(self,embedding_dim,vocab_size,num_rec_layers,dropout,pad_idx,bidirectional=True):\n",
        "    super().__init__()\n",
        "    #self.embedding_dim=embedding_dim\n",
        "    self.embeddings=nn.Embedding(vocab_size,embedding_dim,padding_idx=pad_idx)\n",
        "    self.linear=nn.Linear((2 if bidirectional else 1)*embedding_dim,3)\n",
        "    self.lstm=nn.LSTM(input_size=embedding_dim,num_layers=num_rec_layers,hidden_size=embedding_dim,bidirectional=bidirectional,batch_first=True,dropout=dropout)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.relu=nn.ReLU()\n",
        "    self.bidirectional=bidirectional\n",
        "\n",
        "  def forward(self, input, lengths):\n",
        "    # input: [batch_size, seq_len]\n",
        "\n",
        "    embedded = self.embeddings(input)  # [batch_size, seq_len, embedding_dim]\n",
        "\n",
        "    # Sort by lengths (required by pack_padded_sequence)\n",
        "    lengths_sorted, perm_idx = lengths.sort(0, descending=True)\n",
        "    #print(f'perm_idx:{perm_idx}')\n",
        "    embedded = embedded[perm_idx]\n",
        "    packed_input = pack_padded_sequence(embedded, lengths_sorted.cpu(), batch_first=True)\n",
        "\n",
        "    packed_output, (h_n, c_n) = self.lstm(packed_input)\n",
        "\n",
        "    # Reverse the sort\n",
        "    _, unperm_idx = perm_idx.sort(0)\n",
        "\n",
        "    # h_n: [num_layers * num_directions, batch_size, hidden_size]\n",
        "    # Take the final layer's hidden states\n",
        "    h_n = h_n.view(self.lstm.num_layers, 2 if self.lstm.bidirectional else 1, embedded.size(0), self.lstm.hidden_size)\n",
        "    h_last = h_n[-1]  # last layer\n",
        "    #print(f'h_last.shape:{h_last.shape}')\n",
        "    if self.lstm.bidirectional:\n",
        "        h_last = torch.cat((h_last[0], h_last[1]), dim=1)  # [batch, 2*hidden]\n",
        "    else:\n",
        "        h_last = h_last.squeeze(0)  # [batch, hidden]\n",
        "\n",
        "    # Unsort back to original order\n",
        "    h_last = h_last[unperm_idx]\n",
        "\n",
        "    out = self.linear(self.dropout(h_last))\n",
        "    return self.relu(out)\n"
      ],
      "metadata": {
        "id": "uWzq13OU02aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training"
      ],
      "metadata": {
        "id": "8WGnarWkkNYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "checkpoint_path='/content/drive/MyDrive/SentimentAnalysis/checkpoint.pth'\n",
        "legend={0:'Negative',1:'Neutral',2:'Positive'}\n",
        "pad_idx = tokenizer.pad_token_id\n",
        "model=SentimentAnalyser(embedding_dim=256,vocab_size=tokenizer.vocab_size,num_rec_layers=2,\n",
        "dropout=0.5,pad_idx=pad_idx, bidirectional=True).to(device)\n",
        "#checkpoint=torch.load(checkpoint_path)\n",
        "#model.load_state_dict(checkpoint['model_state_dict'])\n",
        "softmax=nn.Softmax(dim=1)"
      ],
      "metadata": {
        "id": "6byQaQny9x5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs,lr=100,3e-4\n",
        "# Learning rate scheduler: reduce LR by gamma every 'step_size' epochs\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=lr)\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)  # halve LR every 5 epochs\n",
        "loss_fn=nn.CrossEntropyLoss()\n",
        "#min_loss=checkpoint['min_loss']\n",
        "min_loss=np.inf\n",
        "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ],
      "metadata": {
        "id": "JQ6TukMDnPVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(outputs,labels,training=False):\n",
        "  if not training: return (outputs==labels).sum()/len(labels)\n",
        "  return (softmax(outputs).argmax(dim=1)==labels).sum()/len(labels)"
      ],
      "metadata": {
        "id": "QvOB4p1DntOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_losses=[]\n",
        "for epoch in range(1,num_epochs+1):\n",
        "  batch_losses=[]\n",
        "  model.train()\n",
        "  for idx,batch in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    inputs=batch['input_ids'].to(device)\n",
        "    labels=batch['labels'].to(device)\n",
        "    logits=model(input=inputs,lengths=batch['attention_mask'].sum(dim=1))\n",
        "    loss=loss_fn(logits,labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    batch_losses.append(loss.item())\n",
        "    if (epoch==1 or epoch%10==0) and (idx+1)%100==0:\n",
        "      print(f'Epoch {epoch}, Batch {idx+1}, Batch Loss:{batch_losses[-1]:.8f} and accuracy:{accuracy(logits,labels,training=True)*100}%')\n",
        "  epoch_losses.append(np.mean(batch_losses))\n",
        "  scheduler.step()\n",
        "\n",
        "  model.eval()\n",
        "  if epoch%5==0:\n",
        "   with torch.no_grad():\n",
        "    for idx,batch in enumerate(val_loader):\n",
        "      inputs=batch['input_ids'].to(device)\n",
        "      labels=batch['labels'].to(device)\n",
        "      logits=model(inputs,batch['attention_mask'].sum(dim=1))\n",
        "      loss=loss_fn(logits,labels)\n",
        "      print(f'Epoch {epoch}, Batch {idx+1}, Validation Batch Loss:{loss.item():.4f}, and accuracy:{accuracy(logits,labels,training=True)*100}%')\n",
        "\n",
        "  if epoch==1 or epoch%10==0:\n",
        "    print(f'Epoch {epoch}, Learning Rate={scheduler.get_last_lr()[0]}')\n",
        "  print(f'Epoch {epoch}, Epoch Loss:{epoch_losses[-1]:.4f}')\n",
        "  if epoch_losses[-1]<min_loss:\n",
        "    min_loss=epoch_losses[-1]\n",
        "    checkpoint={'model_state_dict':model.state_dict(),'optimizer_state_dict':optimizer.state_dict(),'min_loss':epoch_losses[-1].item()}\n",
        "    torch.save(checkpoint,checkpoint_path)\n",
        "    print('Model saved!')\n",
        "  print()"
      ],
      "metadata": {
        "id": "wcPkwmVunrMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Testing"
      ],
      "metadata": {
        "id": "4cEepm7EkQ8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader=DataLoader(\n",
        "    tokenized_dataset[\"test\"],\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "gTdVXC_qC99j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "for idx,batch in enumerate(test_loader):\n",
        "  if (idx+1)%10!=0:continue\n",
        "  with torch.no_grad():\n",
        "    logits=model(batch['input_ids'].to(device),batch['attention_mask'].sum(dim=1))\n",
        "    logits=softmax(logits).argmax(dim=1)\n",
        "    #print(f'Logits shape: {logits.shape}')\n",
        "    print(f\"Batch: {idx+1}, Accuracy: {accuracy(logits,batch['labels'].to(device))*100}%\")"
      ],
      "metadata": {
        "id": "cqQtUnWZBIr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def determine_sentiment(output,legend):\n",
        "  return legend[output.argmax(dim=1).item()]"
      ],
      "metadata": {
        "id": "M9H5oUMEhIn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_inputs=[\"I am unhappy with your behaviour!\",\"This is a good outcome!\",\"I am perfectly fit and fine.\"]\n",
        "for custom_input in custom_inputs:\n",
        " print(f'{custom_input}: ',end='')\n",
        " tokenized=tokenizer(custom_input,return_tensors='pt')\n",
        " print(determine_sentiment(model(tokenized['input_ids'].to(device),tokenized['attention_mask'].sum(dim=1)),legend))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9LyUEAygIvD",
        "outputId": "d9e6cded-4019-46fd-b419-0ea06ddb82ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am unhappy with your behaviour!: Negative\n",
            "This is a good outcome!: Positive\n",
            "I am perfectly fit and fine.: Neutral\n"
          ]
        }
      ]
    }
  ]
}