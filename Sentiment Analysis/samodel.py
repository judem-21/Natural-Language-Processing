# -*- coding: utf-8 -*-
"""SAModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/162Wd27lvV45cR2AzB64QoHgVuAhrhrJu
"""

!pip install datasets -q

import pandas as pd
from collections import Counter
import torch
import torch.nn as nn
import torch.optim as optim
from datasets import load_dataset
from transformers import DataCollatorWithPadding
from torch.utils.data import DataLoader
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from torch.optim.lr_scheduler import StepLR

dataset = load_dataset("Sp1786/multiclass-sentiment-analysis-dataset")

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

def filter_none(example):
    return example["text"] is not None

dataset['test'] = dataset['test'].filter(filter_none)
dataset['validation']=dataset['validation'].filter(filter_none)
dataset['train']=dataset['train'].filter(filter_none)

def tokenize(batch):
    tokenised = tokenizer(text=batch["text"], truncation=True)
    tokenised["labels"] = batch["label"]
    return tokenised

tokenized_dataset = dataset.map(tokenize, batched=True)
tokenized_dataset = tokenized_dataset.remove_columns(['id', 'text', 'sentiment'])

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="pt")
train_loader = DataLoader(
    tokenized_dataset["train"],
    batch_size=32,
    shuffle=True,
    collate_fn=data_collator
)

for batch in train_loader:
    print({k: v.shape for k, v in batch.items()})  # Shows input_ids, attention_mask shapes
    print(f'Input Ids:{batch["input_ids"]}')
    print(f'Attention Mask:{batch["attention_mask"]}')
    print(f'Labels:{batch["labels"]}')
    break

class SentimentAnalyser(nn.Module):
  def __init__(self,embedding_dim,vocab_size,num_rec_layers,dropout,pad_idx,bidirectional=True):
    super().__init__()
    #self.embedding_dim=embedding_dim
    self.embeddings=nn.Embedding(vocab_size,embedding_dim,padding_idx=pad_idx)
    self.linear=nn.Linear(2*embedding_dim if bidirectional else embedding_dim,3)
    self.lstm=nn.LSTM(input_size=embedding_dim,num_layers=num_rec_layers,hidden_size=embedding_dim,bidirectional=bidirectional,batch_first=True,dropout=dropout)
    self.dropout=nn.Dropout(dropout)

  def forward(self,input):
    input_embeddings=self.embeddings(input)
    out,(_,_)=self.lstm(input_embeddings)
    out=self.linear(self.dropout(out[:,-1,:]))
    return out

model=SentimentAnalyser(embedding_dim=512,vocab_size=tokenizer.vocab_size,num_rec_layers=4,dropout=0.5,pad_idx=pad_idx).to(device)

device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
num_epochs,lr=1000,0.1
pad_idx = tokenizer.pad_token_id
# Learning rate scheduler: reduce LR by gamma every 'step_size' epochs
optimizer=torch.optim.Adam(model.parameters(),lr=lr)
scheduler = StepLR(optimizer, step_size=10, gamma=0.5)  # halve LR every 5 epochs
loss_fn=nn.CrossEntropyLoss()

epoch_losses=[]
for epoch in range(1,num_epochs+1):
  batch_losses=[]
  for idx,batch in enumerate(train_loader):
    optimizer.zero_grad()
    logits=model(batch['input_ids'].to(device))
    loss=loss_fn(logits,batch['labels'].to(device))
    loss.backward()
    optimizer.step()
    batch_losses.append(loss.item())
    if (epoch==1 or epoch%100==0) and (idx+1)%100==0:print(f'Epoch {epoch}, Batch {idx+1}, Batch Loss:{batch_losses[-1]:.8f}')
  epoch_losses.append(np.mean(batch_losses))
  scheduler.step()
  print(f'Epoch:{epoch}, Learning Rate={scheduler.get_last_lr()[0]}')
  if epoch==1 or epoch%100==0:print(f'Epoch:{epoch}, Epoch Loss:{epoch_losses[-1]:.4f}')